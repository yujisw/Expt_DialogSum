#!/usr/bin/env python

import os
import argparse
import datetime
import json
import time
import warnings
from pathlib import Path
from typing import Dict, List

import torch
from tqdm import tqdm

from utils import cal_exact_rouge, calculate_rouge, parse_numeric_n_bool_cl_kwargs
from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu

def run_generate(verbose=True):

    parser = argparse.ArgumentParser()
    parser.add_argument("gen_summary_path", type=str, help="summary file generated by models.")
    parser.add_argument("reference_path", type=str, help="like DATA_DIR/test.target")
    parser.add_argument("--score_path", type=str, required=False, default=None, help="where to save metrics")
    parser.add_argument(
        "--prefix", type=str, required=False, default=None, help="will be added to the begininng of src examples"
    )
    parser.add_argument("--exact", action="store_true")

    # Unspecified args like --num_beams=2 --decoder_start_token_id=4 are passed to model.generate
    args, rest = parser.parse_known_args()
    parsed_args = parse_numeric_n_bool_cl_kwargs(rest)
    if parsed_args and verbose:
        print(f"parsed the following generate kwargs: {parsed_args}")

    # Compute scores
    score_fn = cal_exact_rouge if args.exact else calculate_rouge
    output_lns = [x.rstrip() for x in open(args.gen_summary_path).readlines()]
    reference_lns = [x.rstrip() for x in open(args.reference_path).readlines()][: len(output_lns)]
    scores: dict = score_fn(output_lns, reference_lns)
    # scores.update(runtime_metrics)

    # Compute BLEU method7
    # chencherry = SmoothingFunction()
    # bleu_scores = 0
    # for gen, ref in zip(output_lns, reference_lns):
    #     bleu_scores += sentence_bleu(ref, gen, smoothing_function=chencherry.method7)
    # scores['bleu'] = bleu_scores/len(output_lns)

    if verbose:
        print(scores)

    if args.score_path is not None:
        json.dump(scores, open(args.score_path, "w"))

    return scores

if __name__ == "__main__":
    # Usage for Summarization:
    # python cal_scores.py GEN_SUMMARY $DATA_DIR/test.target --score_path MODEL_DIR/scores.json
    run_generate(verbose=True)
